<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="Docutils 0.19: https://docutils.sourceforge.io/" name="generator"/>
  <meta content="width=device-width,initial-scale=1" name="viewport"/>
  <meta content="ie=edge" http-equiv="x-ua-compatible"/>
  <meta content="Copy to clipboard" name="lang:clipboard.copy"/>
  <meta content="Copied to clipboard" name="lang:clipboard.copied"/>
  <meta content="en" name="lang:search.language"/>
  <meta content="True" name="lang:search.pipeline.stopwords"/>
  <meta content="True" name="lang:search.pipeline.trimmer"/>
  <meta content="No matching documents" name="lang:search.result.none"/>
  <meta content="1 matching document" name="lang:search.result.one"/>
  <meta content="# matching documents" name="lang:search.result.other"/>
  <meta content="[\s\-]+" name="lang:search.tokenizer"/>
  <link crossorigin="" href="https://fonts.gstatic.com/" rel="preconnect"/>
  <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&amp;display=fallback" rel="stylesheet"/>
  <style>
   body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
  </style>
  <link href="../../_static/stylesheets/application.css" rel="stylesheet"/>
  <link href="../../_static/stylesheets/application-palette.css" rel="stylesheet"/>
  <link href="../../_static/stylesheets/application-fixes.css" rel="stylesheet"/>
  <link href="../../_static/fonts/material-icons.css" rel="stylesheet"/>
  <meta content="#3f51b5" name="theme-color"/>
  <script src="../../_static/javascripts/modernizr.js">
  </script>
  <title>
   Moving AI Beyond its Narrow View of Intelligence — Logical Neural Networks Docs
  </title>
  <link href="../../_static/pygments.css" rel="stylesheet" type="text/css"/>
  <link href="../../_static/material.css" rel="stylesheet" type="text/css"/>
  <link href="../../_static/copybutton.css" rel="stylesheet" type="text/css"/>
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js">
  </script>
  <script src="../../_static/jquery.js">
  </script>
  <script src="../../_static/underscore.js">
  </script>
  <script src="../../_static/_sphinx_javascript_frameworks_compat.js">
  </script>
  <script src="../../_static/doctools.js">
  </script>
  <script src="../../_static/sphinx_highlight.js">
  </script>
  <script src="../../_static/clipboard.min.js">
  </script>
  <script src="../../_static/copybutton.js">
  </script>
  <link href="../../genindex.html" rel="index" title="Index"/>
  <link href="../../search.html" rel="search" title="Search"/>
  <link href="../tutorials/tutorials.html" rel="next" title="Tutorials"/>
  <link href="blogs.html" rel="prev" title="Understanding LNNs"/>
 </head>
 <body data-md-color-accent="cyan" data-md-color-primary="blue" dir="ltr">
  <svg class="md-svg">
   <defs data-children-count="0">
    <svg height="448" id="__github" viewbox="0 0 416 448" width="416" xmlns="http://www.w3.org/2000/svg">
     <path d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z" fill="currentColor">
     </path>
    </svg>
   </defs>
  </svg>
  <input class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
  <input class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
  <label class="md-overlay" data-md-component="overlay" for="__drawer">
  </label>
  <a class="md-skip" href="#education/blogs/moving_ai_beyond_its_narrow_view_of_intelligence" tabindex="1">
   Skip to content
  </a>
  <header class="md-header" data-md-component="header">
   <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
     <div class="md-flex__cell md-flex__cell--shrink">
      <a class="md-header-nav__button md-logo" href="../../index.html" title="Logical Neural Networks Docs">
      </a>
     </div>
     <div class="md-flex__cell md-flex__cell--shrink">
      <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer">
      </label>
     </div>
     <div class="md-flex__cell md-flex__cell--stretch">
      <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
       <span class="md-header-nav__topic">
        Logical Neural Networks
       </span>
       <span class="md-header-nav__topic">
        Moving AI Beyond its Narrow View of Intelligence
       </span>
      </div>
     </div>
     <div class="md-flex__cell md-flex__cell--shrink">
      <label class="md-icon md-icon--search md-header-nav__button" for="__search">
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
       <label class="md-search__overlay" for="__search">
       </label>
       <div class="md-search__inner" role="search">
        <form action="../../search.html" class="md-search__form" method="get" name="search">
         <input autocapitalize="off" autocomplete="off" class="md-search__input" data-md-component="query" data-md-state="active" name="q" placeholder="Search" spellcheck="false" type="text"/>
         <label class="md-icon md-search__icon" for="__search">
         </label>
         <button class="md-icon md-search__icon" data-md-component="reset" tabindex="-1" type="reset">
          
         </button>
        </form>
        <div class="md-search__output">
         <div class="md-search__scrollwrap" data-md-scrollfix="">
          <div class="md-search-result" data-md-component="result">
           <div class="md-search-result__meta">
            Type to start searching
           </div>
           <ol class="md-search-result__list">
           </ol>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="md-flex__cell md-flex__cell--shrink">
      <div class="md-header-nav__source">
       <a class="md-source" data-md-source="github" href="https://github.com/IBM/LNN/" title="Go to repository">
        <div class="md-source__icon">
         <svg height="28" viewbox="0 0 24 24" width="28" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
          <use height="24" width="24" xlink:href="#__github">
          </use>
         </svg>
        </div>
        <div class="md-source__repository">
         LNN
        </div>
       </a>
      </div>
     </div>
     <script src="../../_static/javascripts/version_dropdown.js">
     </script>
     <script>
      var json_loc = "../../"versions.json"",
        target_loc = "../../../",
        text = "Versions";
    $( document ).ready( add_version_dropdown(json_loc, target_loc, text));
     </script>
    </div>
   </nav>
  </header>
  <div class="md-container">
   <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
     <ul class="md-tabs__list">
      <li class="md-tabs__item">
       <a class="md-tabs__link" href="../../index.html">
        Logical Neural Networks Docs
       </a>
      </li>
      <li class="md-tabs__item">
       <a class="md-tabs__link" href="../education.html">
        Education
       </a>
      </li>
      <li class="md-tabs__item">
       <a class="md-tabs__link" href="blogs.html">
        Understanding LNNs
       </a>
      </li>
     </ul>
    </div>
   </nav>
   <main class="md-main">
    <div class="md-main__inner md-grid" data-md-component="container">
     <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
      <div class="md-sidebar__scrollwrap">
       <div class="md-sidebar__inner">
        <nav class="md-nav md-nav--primary" data-md-level="0">
         <label class="md-nav__title md-nav__title--site" for="__drawer">
          <a class="md-nav__button md-logo" href="../../index.html" title="Logical Neural Networks Docs">
           <img alt=" logo" height="48" src="../../_static/" width="48"/>
          </a>
          <a href="../../index.html" title="Logical Neural Networks Docs">
           Logical Neural Networks
          </a>
         </label>
         <div class="md-nav__source">
          <a class="md-source" data-md-source="github" href="https://github.com/IBM/LNN/" title="Go to repository">
           <div class="md-source__icon">
            <svg height="28" viewbox="0 0 24 24" width="28" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
             <use height="24" width="24" xlink:href="#__github">
             </use>
            </svg>
           </div>
           <div class="md-source__repository">
            LNN
           </div>
          </a>
         </div>
         <ul class="md-nav__list">
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../introduction.html">
            Logical Neural Networks
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../usage.html">
            Python API
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../papers.html">
            Papers
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../education.html">
            Education
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="blogs.html">
              Understanding LNNs
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../tutorials/tutorials.html">
              Tutorials
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../examples/examples.html">
              Examples
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../videos/videos.html">
              Videos
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../lnn/LNN.html">
            LNN Module
           </a>
          </li>
         </ul>
        </nav>
       </div>
      </div>
     </div>
     <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
      <div class="md-sidebar__scrollwrap">
       <div class="md-sidebar__inner">
        <nav class="md-nav md-nav--secondary">
         <label class="md-nav__title" for="__toc">
          Contents
         </label>
         <ul class="md-nav__list" data-md-scrollfix="">
          <li class="md-nav__item">
           <a class="md-nav__link" href="#education-blogs-moving-ai-beyond-its-narrow-view-of-intelligence--page-root">
            Moving AI Beyond its Narrow View of Intelligence
           </a>
           <nav class="md-nav">
            <ul class="md-nav__list">
             <li class="md-nav__item">
              <a class="md-nav__link" href="#so-what-s-missing-in-today-s-ai">
               So what’s missing in today’s AI?
              </a>
             </li>
             <li class="md-nav__item">
              <a class="md-nav__link" href="#why-a-logical-representation-is-important">
               Why a logical representation is important
              </a>
             </li>
             <li class="md-nav__item">
              <a class="md-nav__link" href="#logical-neural-networks-lnns">
               Logical Neural Networks (LNNs)
              </a>
             </li>
             <li class="md-nav__item">
              <a class="md-nav__link" href="#references">
               References
              </a>
             </li>
            </ul>
           </nav>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__extra_link" href="../../_sources/education/blogs/moving_ai_beyond_its_narrow_view_of_intelligence.md.txt">
            Show Source
           </a>
          </li>
          <li class="md-nav__item" id="searchbox">
          </li>
         </ul>
        </nav>
       </div>
      </div>
     </div>
     <div class="md-content">
      <article class="md-content__inner md-typeset" role="main">
       <section id="moving-ai-beyond-its-narrow-view-of-intelligence">
        <h1 id="education-blogs-moving-ai-beyond-its-narrow-view-of-intelligence--page-root">
         Moving AI Beyond its Narrow View of Intelligence
         <a class="headerlink" href="#education-blogs-moving-ai-beyond-its-narrow-view-of-intelligence--page-root" title="Permalink to this heading">
          ¶
         </a>
        </h1>
        <blockquote>
         <div>
          <p>
           <em>
            We’ve all seen the headlines: “Machine Outsmarts Human”, but can a
computational system truly be smart if it lacks the ability to explain itself
- even to the very researchers that created it?
           </em>
          </p>
         </div>
        </blockquote>
        <p>
         Despite achieving accuracies that surpass human-level performance on narrowly
defined tasks, today’s AI has become handicapped by the very methodology that
brought it fame - Deep Learning. These deep neural structures owe their success
to immense human ingenuity, by encoding knowledge of the world not into
rules, but instead into a complex network of connected neurons within the
model  architecture. This complexity mandates that researchers think
abstractly about how the network behaves, guided by very few interpretable
signals that characterise a model’s behavior.
        </p>
        <p>
         <a class="reference external" href="https://xkcd.com/1838">
          <img alt="ML Systems" class="aligncenter" src="https://www.macloo.com/ai/wp-content/uploads/2020/09/machine_learning_XKCD.png" width="400"/>
         </a>
        </p>
        <section id="so-what-s-missing-in-today-s-ai">
         <h2 id="so-what-s-missing-in-today-s-ai">
          So what’s missing in today’s AI?
          <a class="headerlink" href="#so-what-s-missing-in-today-s-ai" title="Permalink to this heading">
           ¶
          </a>
         </h2>
         <p>
          While “opaque box” models are all the rage right now, they still lack the “per-neuron” symbolic interpretability required for machine learning to generalise to broader tasks. By grounding each neuron’s behavior in an understandable way, researchers can change the way they engage with these complex architectures to be more principled. This also opens up the doors to using ML in regulated environments, where a deployed machine is required to justify its decision making in a way that people can understand. This is where IBM researchers come in. They have managed to merge the well-respected field of logic with neural backpropagation, allowing gradient decent to apply to hand-crafted and noisy knowledge of the world [
          <span class="xref myst">
           1
          </span>
          ].
         </p>
        </section>
        <section id="why-a-logical-representation-is-important">
         <h2 id="why-a-logical-representation-is-important">
          Why a logical representation is important
          <a class="headerlink" href="#why-a-logical-representation-is-important" title="Permalink to this heading">
           ¶
          </a>
         </h2>
         <p>
          Logical operations are arguably human-interpretable, constraining operations on
inputs to behave in a manner that is both predictable and consistent with the
type of gate being used. For example, if our knowledge of the world states
the following:
         </p>
         <blockquote>
          <div>
           <ol class="arabic simple">
            <li>
             <p>
              If it rains then the grass is wet
             </p>
            </li>
           </ol>
          </div>
         </blockquote>
         <p>
          We expect that any decision-making system should be able to apply logical rules
such as modus ponens to our knowledge, i.e. reasoning that the grass is
indeed wet when I know that it is raining - without needing to see a
Bajillion examples first. Simply, the DL has no extrinsic
knowledge, leveraging only a heap of correlations to learn that the two
inputs may be strongly related. But honestly, how can we expect a DL system
to conform to such knowledge without having an explicit handle on neurons
within the hidden layers that encode information. While it is possible to
encode such relations implicitly, there is a computational and therefore
financial and environmental cost to building such systems - which should
really not be there, since our network only needs 3 neurons to compute on
such knowledge:
         </p>
         <img alt="It is raining Implies the grass is wet" class="aligncenter" src="https://raw.githubusercontent.com/IBM/LNN/master/docs/source/education/blogs/raining_implies_wet.png" width="320"/>
         <p>
          Logic also allows us to build high-level decision makers that can reason about
outcomes given only partial information about the world. Lets add some more
knowledge to our model:
         </p>
         <blockquote>
          <div>
           <ol class="arabic simple" start="2">
            <li>
             <p>
              I need to walk on the grass to reach the rosebushes
             </p>
            </li>
            <li>
             <p>
              If the grass is wet then I should not walk on it without rubber boots on
             </p>
            </li>
           </ol>
          </div>
         </blockquote>
         <blockquote>
          <div>
           <p>
            Goal: Trim the rosebushes
           </p>
          </div>
         </blockquote>
         <p>
          Given only the following two pieces of information should be sufficient to know if I can reach my goal:
         </p>
         <blockquote>
          <div>
           <ul class="simple">
            <li>
             <p>
              It is raining
             </p>
            </li>
            <li>
             <p>
              I am not wearing rubber boots
             </p>
            </li>
           </ul>
          </div>
         </blockquote>
         <p>
          Without the ability to reason, even large models would fail to reach simple goals in an efficient manner [
          <span class="xref myst">
           2
          </span>
          ]. Using a logical system would therefore allow an agent to reason that it needs to make certain (interpretable) decisions in order to reach its goal.
         </p>
        </section>
        <section id="logical-neural-networks-lnns">
         <h2 id="logical-neural-networks-lnns">
          Logical Neural Networks (LNNs)
          <a class="headerlink" href="#logical-neural-networks-lnns" title="Permalink to this heading">
           ¶
          </a>
         </h2>
         <p>
          LNNs are a mathematically follow a sound and complete extension to weighted real-valued logic [
          <span class="xref myst">
           3
          </span>
          ], offering a new class of NeuroSymbolic approaches according to Henry Kautz’s taxonomy [
          <span class="xref myst">
           4
          </span>
          ], namely
          <code class="docutils literal notranslate">
           <span class="pre">
            Neuro
           </span>
           <span class="pre">
            =
           </span>
           <span class="pre">
            Symbolic
           </span>
          </code>
          . This framework offers a handful of novel features:
         </p>
         <p>
          Knowledge can embedded in a neural network via logical statements using specialised activations to compute logical operations.
A 1-to-1 representation is used whereby each logical symbol is directly encoded by a single neuron, representing the network as an interpretable syntax tree.
The LNN is also implemented using a dynamic graph representation where reasoning is computed via message passing algorithms, allowing new facts and rules to be added to the model on the fly.
With this syntax tree, inferences such as modus ponens require facts at leaf nodes to be updated - facilitated by downward inferences, allowing the network to both handle missing inputs and verify consistent interaction between the rules and facts.
This consistency checking allows LNNs to learn purely by self-supervision, while still allowing everybody’s favourite loss function to join the party.
Using bounds to represent inputs allows the LNN to operate under the open world assumption, also allowing nuance by having ranges of inputs being plausible at a single neuron.
Just like standard NNs, weights are attached to edges and learning via backprop caters to imperfect knowledge of the world.
The system is also end-to-end differentiable, allowing LNNs to play nice with multiple DL systems simultaneously. These DL models can still do what they do best - act as function approximators of hierarchical features that output to a single node, but in this case, the node may be given as an input to an LNN and governed by the logic that is expected of its symbolic behaviour.
         </p>
         <p>
          With all this added functionality, LNNs offer a completely new class of learning-based approaches to the AI toolbox.
         </p>
         <p>
          As the modern economy moves towards embedding real-time computation into every aspect of life, so too will AI follow. The ability to generalise beyond narrow tasks requires deployed systems to simultaneously obey rules of inference while still being robust against an ocean of noisy, unstructured data. It stands to reason [get it?], that having a differentiable white-box model that acts as a symbolic decision-maker is needed for the next generational leap in the field of AI. Perhaps advances like these will move the scientific community forward; towards an intelligence that is both generalisable and understandable.
         </p>
        </section>
        <section id="references">
         <h2 id="references">
          References
          <a class="headerlink" href="#references" title="Permalink to this heading">
           ¶
          </a>
         </h2>
         <ol class="arabic simple">
          <li>
           <p>
            <a class="reference external" href="https://arxiv.org/abs/2006.13155">
             Logical Neural Networks
            </a>
            <a name="ref_1">
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference external" href="https://arxiv.org/abs/2103.02363">
             Reinforcement Learning with External Knowledge by using Logical Neural Networks
            </a>
            <a name="ref_2">
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference external" href="https://arxiv.org/abs/2008.02429">
             Foundations of Reasoning with Uncertainty via Real-valued Logics
            </a>
            <a name="ref_3">
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference external" href="https://vimeo.com/389560858">
             Robert S. Engelmore Memorial Award Lecture: The Third AI Summer - Henry Kautz
            </a>
            <a name="ref_4">
            </a>
           </p>
          </li>
         </ol>
        </section>
       </section>
      </article>
     </div>
    </div>
   </main>
  </div>
  <footer class="md-footer">
   <div class="md-footer-nav">
    <nav class="md-footer-nav__inner md-grid">
     <a class="md-flex md-footer-nav__link md-footer-nav__link--prev" href="blogs.html" rel="prev" title="Understanding LNNs">
      <div class="md-flex__cell md-flex__cell--shrink">
       <i class="md-icon md-icon--arrow-back md-footer-nav__button">
       </i>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
       <span class="md-flex__ellipsis">
        <span class="md-footer-nav__direction">
         Previous
        </span>
        Understanding LNNs
       </span>
      </div>
     </a>
     <a class="md-flex md-footer-nav__link md-footer-nav__link--next" href="../tutorials/tutorials.html" rel="next" title="Tutorials">
      <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
       <span class="md-flex__ellipsis">
        <span class="md-footer-nav__direction">
         Next
        </span>
        Tutorials
       </span>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
       <i class="md-icon md-icon--arrow-forward md-footer-nav__button">
       </i>
      </div>
     </a>
    </nav>
   </div>
   <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
     <div class="md-footer-copyright">
      <div class="md-footer-copyright__highlight">
       © Copyright 2023, IBM Research.
      </div>
      Created using
      <a href="http://www.sphinx-doc.org/">
       Sphinx
      </a>
      5.3.0.
             and
      <a href="https://github.com/bashtage/sphinx-material/">
       Material for
              Sphinx
      </a>
     </div>
    </div>
   </div>
  </footer>
  <script src="../../_static/javascripts/application.js">
  </script>
  <script>
   app.initialize({version: "1.0.4", url: {base: ".."}})
  </script>
 </body>
</html>